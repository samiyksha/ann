import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns


def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def tanh(x):
    return np.tanh(x)

def relu(x):
    return np.maximum(0, x)

def leaky_relu(x, alpha=0.01):
    return np.where(x > 0, x, alpha * x)

def swish(x):
    return x * sigmoid(x)

x = np.linspace(-5, 5, 400)

y_sigmoid = sigmoid(x)
y_tanh = tanh(x)
y_relu = relu(x)
y_leaky_relu = leaky_relu(x)
y_swish = swish(x)

sns.set_style("darkgrid")
#plt.figure(figsize=(10, 6))
plt.plot(x, y_sigmoid, label='Sigmoid', color='blue')
# plt.plot(x, y_tanh, label='Tanh', color='red')
#plt.plot(x, y_relu, label='ReLU', color='green')
#plt.plot(x, y_leaky_relu, label='Leaky ReLU', color='purple')
#plt.plot(x, y_swish, label='Swish', color='orange')

plt.xlabel("Input")
plt.ylabel("Output")
plt.title("Activation Functions in Neural Networks")
plt.legend()
plt.show()
