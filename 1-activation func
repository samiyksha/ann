import numpy as np
import matplotlib.pyplot as plt

def linear(x):
    return x

x = np.linspace(-5,10)
plt.plot(x,linear(x))
plt.axis('tight')
plt.title('Activation Function: linear')
plt.show()

import numpy as np
import matplotlib.pyplot as plt

def sigmoid(x):
    return 1/(1+np.exp(-x))

x = np.linspace(-10,10)
plt.plot(x,sigmoid(x))
plt.axis('tight')
plt.title('Activation Function: sigmoid')
plt.show()

import numpy as np
import matplotlib.pyplot as plt

def tanh(x):
    return np.tanh(x)

x = np.linspace(-20,20)
plt.plot(x,tanh(x))
plt.axis('tight')
plt.title('Activation Function: tanh')
plt.show()

import numpy as np
import matplotlib.pyplot as plt

def relu(x):
    x1=[]
    for i in x:
        if i < 0:
            x1.append(0)
        else:
            x1.append(i)
    return x1

x = np.linspace(-10,10)
plt.plot(x,relu(x))
plt.axis('tight')
plt.title('Activation Function: RELU')
plt.show()

import numpy as np
import matplotlib.pyplot as plt

def softmax(x):
    return np.exp(x) / np.sum(np.exp(x), axis=0)



x = np.linspace(-10,10)
plt.plot(x,softmax(x))
plt.axis('tight')
plt.title('Activation Function: softmax')
plt.show()
